{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d20e31b0-8e21-4d61-a935-0934fb48aced",
   "metadata": {},
   "source": [
    "# Midterm, T/F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eb16b6-5999-4396-844c-7bb43c6f0ff6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##  Lecture 1, Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a991ba35-eb9e-4dc9-9e80-56651a6b2638",
   "metadata": {
    "tags": []
   },
   "source": [
    "##  Lecture 2, Terminology, Baselines, Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd093d4-e0d0-4379-bca5-8f694417f24d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "**2.0**\n",
    "* There are different types of machine learning.\n",
    "    * T\n",
    "* Predicting spam and predicting housing prices are both examples of supervised machine learning.\n",
    "    * T\n",
    "* For problems such as spelling correction, translation, face recognition, spam identification, if you are a domain expert, it’s usually faster and scalable to come up with a robust set of rules manually rather than building a machine learning model.\n",
    "    * F\n",
    "* Google News is likely be using machine learning to organize news.\n",
    "    * T\n",
    "\n",
    "**2.1**\n",
    "* How many examples and features are there in the housing price data above? You can use df.shape to get number of rows and columns in a dataframe.\n",
    "    * `df.shape()`\n",
    "* For each of the following examples what would be the relevant features and what would be the target?\n",
    "    * Sentiment analysis\n",
    "        * TODO\n",
    "    * Fraud detection\n",
    "        * TODO\n",
    "    * Face recognition\n",
    "        * TODO\n",
    "        \n",
    "**2.2**\n",
    "* Which of these are examples of supervised learning?\n",
    "    * Finding groups of similar properties in a real estate data set.\n",
    "        * F\n",
    "    * Predicting real estate prices based on house features like number of rooms, learning from past sales as examples.\n",
    "        * T\n",
    "    * Grouping articles on different topics from different news sources (something like the Google News app).\n",
    "        * F\n",
    "    * Detecting credit card fraud based on examples of fraudulent and non-fraudulent transactions.\n",
    "        * T\n",
    "         \n",
    "**2.3**\n",
    "* Which of these are examples of classification and which ones are of regression?\n",
    "    * Predicting the price of a house based on features such as number of bedrooms and the year built.\n",
    "        * Reg\n",
    "    * Predicting if a house will sell or not based on features like the price of the house, number of rooms, etc.\n",
    "        * Class\n",
    "    * Predicting percentage grade in CPSC 330 based on past grades.\n",
    "        * Reg\n",
    "    * Predicting whether you should bicycle tomorrow or not based on the weather forecast.\n",
    "        * Class\n",
    "\n",
    "**2.4**\n",
    "* Order the steps below to build ML models using sklearn.\n",
    "    1. Create X and Y\n",
    "    2. Create model\n",
    "    3. Fit\n",
    "    4. Score\n",
    "    5. Predict\n",
    "* predict takes only X as argument whereas fit and score take both X and y as arguments. True or False.\n",
    "    * T\n",
    "* Have you ever played 20-questions game? If yes, think about how do you decide what question to ask next?\n",
    "    * Use all previous questions\n",
    "\n",
    "**2.5**\n",
    "* Should change in features (i.e., binarizing features above) change DummyClassifier predictions?\n",
    "    * No feature changes matter to DummyClassifier\n",
    "* For the decision tree algorithm to work, the feature values must be numeric.\n",
    "    * ~~Yes; but categories can just be numbers~~\n",
    "    * **ACTUALLY NO,** can be categories\n",
    "* For the decision tree algorithm to work, the target values must be numeric.\n",
    "    * No\n",
    "* The decision tree algorithm creates balanced decision trees.\n",
    "    * No"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27db8ac9-ff1d-4e8b-bf75-f3efd7ee7e4c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Lecture 3, ML Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9781bc2e-f4ce-4418-8576-055a1ccfdcb0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "**3.1**\n",
    "1. A decision tree model with no depth is likely to perform very well on the deployment data.\n",
    "    * No\n",
    "2. Data splitting helps us generalize our model better.\n",
    "    * ~~Yes.~~ Data splitting helps us **assess,** the generalization itself.\n",
    "3. Deployment data is used at the very end and only scored once.\n",
    "    * No, that's test data ... you can't score on deployment.\n",
    "4. Validation data could be used for hyperparameter optimization.\n",
    "    * Yes\n",
    "\n",
    "**3.2**\n",
    "1. Why you can typically expect E_train < E_validation < E_test < E_deployment\n",
    "    * True; our model has seen our training data is fitted to it; also see validation subsets in cross-validation (biased towards it); test data is from the same data set as train so likely more related. Deployment is completely unknown.\n",
    "2. Discuss the consequences of not shuffling before splitting the data in train_test_split\n",
    "    * We could end up with a really bad split (i.e., if the data is sorted).\n",
    "    \n",
    "**3.3**\n",
    "1. k fold CV calls fit k times. True or false?\n",
    "    * fit-score k times, yes\n",
    "    * Does it fit on optimal model in the end? No\n",
    "2. We use cross-validation to improve model performance. True or False?\n",
    "    * **False**\n",
    "    * Assess how well our model will generalize to test, deployment\n",
    "3. Discuss advantages and disadvantages of cross-validation.\n",
    "    * Advantage: we're able to assess our model before accessing our test set; can tune hyperparameters\n",
    "    * Disadvantage: slow, less training data in each fold\n",
    "    \n",
    "**3.4**\n",
    "1. If the mean train accuracy is much higher than the mean cross-validation accuracy.\n",
    "    * Overfit\n",
    "2. If the mean train accuracy and the mean cross-validation accuracy are both low and relatively similar in value.\n",
    "    * Underfit\n",
    "3. Decision tree with no limit on the depth.\n",
    "    * Overfit\n",
    "4. Decision stump on a complicated classification problem.\n",
    "    * Underfit\n",
    "\n",
    "**3.5**\n",
    "1. In supervised learning, the training error is always lower than the validation error.\n",
    "    * False\n",
    "2. The fundamental tradeoff of ML states that as training error goes down, validation error goes up.\n",
    "    * ~~Overfitting; true~~\n",
    "    * False ... nothing about model complexity\n",
    "    * E_valid does go up because we're overfitting train\n",
    "3. More \"complicated\" models are more likely to overfit than \"simple\" ones.\n",
    "    * True\n",
    "4. If we had an infinite amount of training data, overfitting would not be a problem.\n",
    "    * True\n",
    "5. If our training error is extremely low, we are likely to be overfitting.\n",
    "    * True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb652db-7eca-45ce-a49d-97da5139b743",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Lecture 4: kNN and SVM RBF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85674922-52e9-46d6-9ba7-f46ade555bbe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "**4.1** \n",
    "1. Analogy-based models find examples from the test set that are most similar to the query example we are predicting.\n",
    "    * False, from the train set\n",
    "2. A dataset with 10 dimensions is considered low dimensional.\n",
    "    * True; 20 is low, 1000 is medium, 100,000 is high\n",
    "3. Euclidean distance will always have a positive value.\n",
    "    * True\n",
    "    \n",
    "**4.2**\n",
    "Find Euclidean distance.\n",
    "\n",
    "**4.3**\n",
    "1. Unlike with decision trees, with k-NNs most of the work is done at the predict stage.\n",
    "    * True\n",
    "    * \"Lazy\"\n",
    "2. With k-NN, setting the hyperparameter to larger values typically reduces training error.\n",
    "    * False\n",
    "    * Lower ks is overfit; only consider 1 neighbor\n",
    "    * Higher ks is underfit; consider too many ks (end up with just DummyClassifier)\n",
    "3. Similar to decision trees, k-NNs finds a small set of good features.\n",
    "    * No\n",
    "    * Considers all the features, all the time\n",
    "* In k-NN, the classification of the closest neighbour to the test example always contributes the most to the prediction.\n",
    "    * All have equal vote\n",
    "\n",
    "**4.4**\n",
    "1. Why set diagonal entries to 0?\n",
    "    * Distance to self shouldn't be included, always 0\n",
    "2. Toy data set\n",
    "    * \n",
    "\n",
    "**4.5**\n",
    "1. k-NN may perform poorly in high-dimensional space (say, d > 1000).\n",
    "    * True\n",
    "2. Similar to KNN, SVM with RBF kernel is a non-parametric model.\n",
    "    * True \n",
    "        * Non-parametric if we have to store O(n) things to make a prediction\n",
    "        * SVM only stores key examples, so should be less... non-parametric still though\n",
    "3. In SVM RBF, removing a non-support vector would not change the decision boundary.\n",
    "    * True\n",
    "4. In sklearn’s SVC classifier, large values of gamma tend to result in higher training score but probably lower validation score.\n",
    "    * Exactly; overfit with high gamma because increase model complexity\n",
    "5. If we increase both gamma and C, we can’t be certain if the model becomes more complex or less complex.\n",
    "    * Both increase complexity (I think)\n",
    "    * Larger gamma, more complex\n",
    "    * Larger C, more complex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934bf980-5220-43a4-8896-0586fa2609bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Lecture 5: Preprocessing and sklearn pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f388e3-6e34-4934-9368-cd3c33f14ae7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "**5.0**\n",
    "\n",
    "1. StandardScaler ensures a fixed range (i.e., minimum and maximum values) for the features.\n",
    "    * Sets mu to 0 and std dev to 1, so we have a fixed range\n",
    "2. StandardScaler calculates mean and standard deviation for each feature separately.\n",
    "    * Yes; each feature done separately\n",
    "3. In general, it’s a good idea to apply scaling on numeric features before training k-NN or SVM RBF models.\n",
    "    * Yes; need to consider dimensions fairly in distance calculation\n",
    "\n",
    "1. The transformers such as StandardScaler or SimpleImputer in scikit-learn return a dataframe with transformed features.\n",
    "    * False; an ndarray rather than a dataframe (for transform)\n",
    "2. The transformed feature values might be hard to interpret for humans.\n",
    "    * Yes, of course; this is a disadvantage of scaling\n",
    "3. After applying SimpleImputer The transformed data has a different shape than the original data.\n",
    "    * No; we're just filling in blanks\n",
    "    \n",
    "    \n",
    "Pipelines\n",
    "\n",
    "1. You can “glue” together imputation and scaling of numeric features and scikit-learn classifier object within a single pipeline.\n",
    "    * Yes\n",
    "2. You can “glue” together scaling of numeric features, one-hot encoding of categorical features, and scikit-learn classifier object within a single pipeline.\n",
    "    * No; need column transformer\n",
    "3. Once you have a scikit-learn pipeline object you can call fit, predict, and score on it.\n",
    "    * True; as long as the last thing isn't a transformer\n",
    "\n",
    "More\n",
    "\n",
    "1. You can carry out data splitting within scikit-learn pipeline.\n",
    "    * No; we split and pass pipe to each fold\n",
    "2. We have to be careful of the order we put each transformation and model in a pipeline.\n",
    "    * Yes; end with model\n",
    "3. Pipelines will fit and transform on the training fold and only transform on the validation fold during cross-validation.\n",
    "    * Yes; this is the point!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7989f0-0659-42bb-842e-67a985a53499",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Lecture 6: ColumnTransformer and Text Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0afd31-2b6b-4cc4-abf1-de324a2a025c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "**6.0**\n",
    "\n",
    "1. You could carry out cross-validation by passing a ColumnTransformer object to cross_validate.\n",
    "    * Yes; it will run fit_transform and transform appropriately\n",
    "2. After applying column transformer, the order of the columns in the transformed data has to be the same as the order of the columns in the original data.\n",
    "    * General order is the same; we might add new columns\n",
    "3. After applying a column transformer, the transformed data is always going to be of different shape than the original data.\n",
    "    * No; what if we just scaled? Or imputed?\n",
    "4. When you call fit_transform on a ColumnTransformer object, you get a numpy ndarray.\n",
    "    * True; we need to re-attach column names and convert to a data frame\n",
    "\n",
    "\n",
    "1. handle_unknown=\"ignore\" would treat all unknown categories equally.\n",
    "    * Yes; all things unknown become a part of some \"other\" category\n",
    "2. Creating groups of rarely occurring categories might overfit the model.\n",
    "    * False; doesn't overfit... moves towards underfit\n",
    "    \n",
    "CountVectorizer\n",
    "\n",
    "1. As you increase the value for max_features hyperparameter of CountVectorizer the training score is likely to go up.\n",
    "    * More words, overfit\n",
    "2. Suppose you are encoding text data using CountVectorizer. If you encounter a word in the validation or the test split that’s not available in the training data, we’ll get an error.\n",
    "    * If it's not in bag of words, it's just ignored!\n",
    "    * False\n",
    "3. max_df hyperparameter of CountVectorizer can be used to get rid of most frequently occurring words from the dictionary.\n",
    "    * Yes; ignore super common words\n",
    "4. In the code below, inside cross_validate, each fold might have slightly different number of features (columns) in the fold.\n",
    "    * Could vary words available in a split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364ec307-b14d-4595-8469-73a2a21c72bb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Lecture 7: Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67fa05d-9e62-40b7-aa93-10eaa250fed0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "**7.0** \n",
    "\n",
    "1. Increasing the hyperparameter alpha of Ridge is likely to decrease model complexity.\n",
    "    * True; this is the exception \n",
    "2. Ridge can be used with datasets that have multiple features.\n",
    "    * Yes; works for n-dimensions\n",
    "3. With Ridge, we learn one coefficient per training example.\n",
    "    * False; we learn it per feature\n",
    "4. If you train a linear regression model on a 2-dimensional problem (2 features), the model will be a two dimensional plane.\n",
    "    * Yes for **linear regression** since we're predicting a numerical value\n",
    "    * (For logistic regression, predicting a category based on thresholds, this is not true... it's instead d-1 because we only need boundary)\n",
    "\n",
    "1. Suppose you have trained a linear model on an unscaled data. The coefficients of the linear model have the following interpretation: if coefficient is large, that means a change in feature has a large impact on the prediction.\n",
    "    * Not necessarily because $j$ could be artificially large due to it's scale\n",
    "2. Suppose the scaled feature value of NOX feature above is negative. The prediction will still be inversely proportional to NOX; as NOX gets bigger, the median house value gets smaller.\n",
    "    * Depends on the coefficient; in this case, it's true since coeff < 0\n",
    "    \n",
    "    \n",
    "1. Increasing C in LogReg increases complexity\n",
    "    * True\n",
    "2. Unlike with Ridge regression, coefficients are not interpretable with logistic regression.\n",
    "    * Coefficients are interpretable\n",
    "3. The raw output score can be used to calculate the probability score for a given prediction.\n",
    "    * Using sigmoid function, this works\n",
    "4. For linear classifier trained on d features, the decision boundary is a d-1 dimensional hyperplane.\n",
    "    * True\n",
    "5. A linear model is likely to be uncertain about the data points close to the decision boundary.\n",
    "    * True\n",
    "6. Similar to decision trees, conceptually logistic regression should be able to work with categorical features.\n",
    "    * No; can only use pre-processed categorical features\n",
    "7. Scaling might be a good idea in the context of logistic regression.\n",
    "    * Yes; same thing applies as normal regression\n",
    "\n",
    "1. Most important features using KNNs?\n",
    "    * Not always possible\n",
    "2. \" for decision trees?\n",
    "    * Yes; number of nodes it's in\n",
    "    * How much it splits the data, i.e., if it's higher in the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eceabcc-7e14-4788-8f70-23bc445b7e9f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Lecture 8: Hyperparameter and Optimization Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43170ab0-3b06-48ef-b254-667ad5977ce9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "**8.0**\n",
    "\n",
    "1. If you get optimal results at the edges of your parameter grid, it might be a good idea to adjust the range of values in your parameter grid.\n",
    "    * True\n",
    "2. Grid search is guaranteed to find best hyperparameters values.\n",
    "    * False\n",
    "3. It is possible to get different hyperparameters in different runs of RandomizedSearchCV.\n",
    "    * True\n",
    "4. Suppose you have 10 hyperparameters and each takes 4 values. If you run RandomizedSearchCV with this parameter grid, how many cross-validation experiments it would carry out?\n",
    "    * Depends on `n_iter`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fc7293-a325-4271-a2ec-d30775e22f89",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Lecture 9: Classification Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07243ebc-8ed9-4c5a-88b9-3b0ffbb11b75",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "**9.0** \n",
    "\n",
    "1. In medical diagnosis, false positives are more damaging than false negatives (assume “positive” means the person has a disease, “negative” means they don’t).\n",
    "    * False negatives bad for medical\n",
    "2. In spam classification, false positives are more damaging than false negatives (assume “positive” means the email is spam, “negative” means they it’s not).\n",
    "    * False positives bad for spam\n",
    "3. In the medical diagnosis, high recall is more important than high precision.\n",
    "    * Yes; high recall with FN\n",
    "4. If method A gets a higher accuracy than method B, that means its precision is also higher.\n",
    "    * No\n",
    "5. If method A gets a higher accuracy than method B, that means its recall is also higher.\n",
    "    * No\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bcbde6-2184-4265-ac57-85bcab9d7745",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Lecture 10: Regression Metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f89547-d6e8-4297-9c9f-6ae88eafe714",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "**10.0**\n",
    "1. Price per square foot would be a good feature to add in our X.\n",
    "    * No, based on the target\n",
    "2. The alpha hyperparameter of Ridge has similar interpretation of C hyperparameter of LogisticRegression; higher alpha means more complex model.\n",
    "    * $\\alpha$ is the opposite\n",
    "3. In regression, one should use MAPE instead of MSE when relative (percent) error matters more than absolute error.\n",
    "    * True\n",
    "4. A lower RMSE value indicates a better model.\n",
    "    * True\n",
    "5. We can use still use precision and recall for regression problems but now we have other metrics we can use as well.\n",
    "    * False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9011df2f-31d8-432b-abd1-6ac60a83b447",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Lecture 11: Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa7038f-7a92-4777-aebe-801ae56c029d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "**11.0**\n",
    "\n",
    "1. Every tree in a random forest uses a different bootstrap sample of the training set.\n",
    "    * True\n",
    "2. To train a tree in a random forest, we first randomly select a subset of features. The tree is then restricted to only using those features.\n",
    "    * False -- at each node\n",
    "3. A reasonable implementation of predict_proba for random forests would be for each tree to “vote” and then normalize these vote counts into probabilities.\n",
    "    * True\n",
    "4. Increasing the hyperparameter max_features (the number of features to consider for a split) makes the model more complex and moves the fundamental tradeoff toward lower training error.\n",
    "    * Yes\n",
    "5. A random forest with only one tree is likely to get a higher training error than a decision tree of the same depth.\n",
    "    * True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9ae952-d17f-42d6-bfdf-af27b94213b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Lecture 12: Feature Importances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ec8a71-0297-482d-8942-0e428bddf8a8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "1. Train random forest on binary classification. Value of 0.580 given by `feature_importances_` means increasing `feat1` moves toward positive class.\n",
    "    * No; not necessarily positive\n",
    "2. `eli5` for non-sklearn\n",
    "    * Yes\n",
    "3. With SHAP, only explain predictions on train\n",
    "    * No... can be used for any prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
